{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1 epoch: avg losses 75268.9830 68049.2556 0.0000 0.0000 elapsed 17.3707 seconds\n",
                        "2 epoch: avg losses 59315.9411 52032.3614 0.0000 0.0000 elapsed 6.5214 seconds\n",
                        "3 epoch: avg losses 57603.2121 42406.6290 0.0000 0.0000 elapsed 6.2148 seconds\n",
                        "4 epoch: avg losses 55869.6487 33188.3553 0.0000 0.0000 elapsed 6.3397 seconds\n",
                        "5 epoch: avg losses 54771.6093 25805.0103 0.0000 0.0000 elapsed 6.6000 seconds\n",
                        "6 epoch: avg losses 53966.4499 21615.3272 0.0000 0.0000 elapsed 6.5737 seconds\n",
                        "7 epoch: avg losses 53433.4453 17246.6037 0.0000 0.0000 elapsed 6.5423 seconds\n",
                        "8 epoch: avg losses 52960.6305 14089.0966 0.0000 0.0000 elapsed 6.5758 seconds\n",
                        "9 epoch: avg losses 52580.4841 12004.3507 0.0000 0.0000 elapsed 6.5491 seconds\n",
                        "10 epoch: avg losses 52233.1017 10418.8748 0.0000 0.0000 elapsed 6.4991 seconds\n",
                        "11 epoch: avg losses 51889.0660 9002.3251 0.0000 0.0000 elapsed 6.5676 seconds\n",
                        "12 epoch: avg losses 51623.8438 7724.5595 0.0000 0.0000 elapsed 6.5274 seconds\n",
                        "13 epoch: avg losses 51273.1707 6567.8764 0.0000 0.0000 elapsed 6.5195 seconds\n",
                        "14 epoch: avg losses 51023.8133 5578.9253 0.0000 0.0000 elapsed 6.5046 seconds\n",
                        "15 epoch: avg losses 50762.3991 4727.6894 0.0000 0.0000 elapsed 6.4935 seconds\n",
                        "16 epoch: avg losses 50507.7133 4088.1558 0.0000 0.0000 elapsed 6.5283 seconds\n",
                        "17 epoch: avg losses 50224.7816 3595.5147 0.0000 0.0000 elapsed 6.5229 seconds\n",
                        "18 epoch: avg losses 50005.0707 3202.0545 0.0000 0.0000 elapsed 6.5303 seconds\n",
                        "19 epoch: avg losses 49754.3404 2877.2984 0.0000 0.0000 elapsed 6.5085 seconds\n",
                        "20 epoch: avg losses 49531.7654 2595.0982 0.0000 0.0000 elapsed 6.5139 seconds\n",
                        "21 epoch: avg losses 49314.1987 2360.7754 0.0000 0.0000 elapsed 6.5078 seconds\n",
                        "22 epoch: avg losses 49026.8159 2142.2336 0.0000 0.0000 elapsed 6.5160 seconds\n",
                        "23 epoch: avg losses 48823.9602 1977.7743 0.0000 0.0000 elapsed 6.5549 seconds\n",
                        "24 epoch: avg losses 48627.1332 1815.0294 0.0000 0.0000 elapsed 6.5248 seconds\n",
                        "25 epoch: avg losses 48434.8470 1679.8894 0.0000 0.0000 elapsed 6.5742 seconds\n",
                        "26 epoch: avg losses 48249.8841 1545.1157 0.0000 0.0000 elapsed 6.5759 seconds\n",
                        "27 epoch: avg losses 48057.6582 1437.8719 0.0000 0.0000 elapsed 6.5642 seconds\n",
                        "28 epoch: avg losses 47866.7587 1316.5112 0.0000 0.0000 elapsed 6.6069 seconds\n",
                        "29 epoch: avg losses 47662.5277 1216.6952 0.0000 0.0000 elapsed 6.5260 seconds\n",
                        "30 epoch: avg losses 47495.4546 1103.5527 0.0000 0.0000 elapsed 6.5224 seconds\n",
                        "31 epoch: avg losses 47306.2732 1000.6294 0.0000 0.0000 elapsed 6.5053 seconds\n",
                        "32 epoch: avg losses 47127.5019 899.0605 0.0000 0.0000 elapsed 6.5504 seconds\n",
                        "33 epoch: avg losses 46934.6476 833.2567 0.0000 0.0000 elapsed 6.5644 seconds\n",
                        "34 epoch: avg losses 46765.0954 750.8115 0.0000 0.0000 elapsed 6.5678 seconds\n",
                        "35 epoch: avg losses 46586.6534 664.2435 0.0000 0.0000 elapsed 6.5210 seconds\n",
                        "36 epoch: avg losses 46410.5682 616.5214 0.0000 0.0000 elapsed 6.5112 seconds\n",
                        "37 epoch: avg losses 46235.5419 543.1900 0.0000 0.0000 elapsed 6.5475 seconds\n",
                        "38 epoch: avg losses 46058.5324 500.1446 0.0000 0.0000 elapsed 6.5427 seconds\n",
                        "39 epoch: avg losses 45891.4661 450.8274 0.0000 0.0000 elapsed 6.1003 seconds\n",
                        "40 epoch: avg losses 45702.4884 389.9694 0.0000 0.0000 elapsed 6.0325 seconds\n",
                        "41 epoch: avg losses 45528.5595 360.3745 0.0000 0.0000 elapsed 6.0232 seconds\n",
                        "42 epoch: avg losses 45332.7813 320.4255 0.0000 0.0000 elapsed 6.0334 seconds\n",
                        "43 epoch: avg losses 45179.6303 292.9011 0.0000 0.0000 elapsed 6.0931 seconds\n",
                        "44 epoch: avg losses 45028.8248 261.3036 0.0000 0.0000 elapsed 6.0412 seconds\n",
                        "45 epoch: avg losses 44861.8743 243.6265 0.0000 0.0000 elapsed 6.0079 seconds\n",
                        "46 epoch: avg losses 44699.0841 218.5598 0.0000 0.0000 elapsed 6.0239 seconds\n",
                        "47 epoch: avg losses 44564.8384 192.7475 0.0000 0.0000 elapsed 6.0139 seconds\n",
                        "48 epoch: avg losses 44396.4937 171.6038 0.0000 0.0000 elapsed 6.0217 seconds\n",
                        "49 epoch: avg losses 44264.9698 162.7297 0.0000 0.0000 elapsed 6.0227 seconds\n",
                        "50 epoch: avg losses 44099.5581 150.4060 0.0000 0.0000 elapsed 6.0621 seconds\n",
                        "51 epoch: avg losses 43952.8891 131.2284 0.0000 0.0000 elapsed 6.0531 seconds\n",
                        "52 epoch: avg losses 43798.2947 126.4680 0.0000 0.0000 elapsed 6.0234 seconds\n",
                        "53 epoch: avg losses 43683.1720 109.1928 0.0000 0.0000 elapsed 6.0334 seconds\n",
                        "54 epoch: avg losses 43516.9322 103.1359 0.0000 0.0000 elapsed 6.0460 seconds\n",
                        "55 epoch: avg losses 43373.8761 93.6615 0.0000 0.0000 elapsed 6.0851 seconds\n",
                        "56 epoch: avg losses 43213.4446 80.6786 0.0000 0.0000 elapsed 6.0237 seconds\n",
                        "57 epoch: avg losses 43075.5467 77.4455 0.0000 0.0000 elapsed 6.0230 seconds\n",
                        "58 epoch: avg losses 42955.2681 69.7935 0.0000 0.0000 elapsed 6.0642 seconds\n",
                        "59 epoch: avg losses 42801.2677 62.3712 0.0000 0.0000 elapsed 6.0515 seconds\n",
                        "60 epoch: avg losses 42672.4256 57.7582 0.0000 0.0000 elapsed 6.0636 seconds\n",
                        "61 epoch: avg losses 42545.0847 53.4173 0.0000 0.0000 elapsed 6.0121 seconds\n",
                        "62 epoch: avg losses 42381.7914 48.8842 0.0000 0.0000 elapsed 5.9915 seconds\n",
                        "63 epoch: avg losses 42260.1712 44.4917 0.0000 0.0000 elapsed 6.0434 seconds\n",
                        "64 epoch: avg losses 42137.0223 42.0012 0.0000 0.0000 elapsed 6.0352 seconds\n",
                        "65 epoch: avg losses 42011.5059 38.7166 0.0000 0.0000 elapsed 5.8724 seconds\n",
                        "66 epoch: avg losses 41908.1925 35.1243 0.0000 0.0000 elapsed 5.8827 seconds\n",
                        "67 epoch: avg losses 41780.3840 33.8093 0.0000 0.0000 elapsed 5.9518 seconds\n",
                        "68 epoch: avg losses 41670.7106 31.1229 0.0000 0.0000 elapsed 6.0136 seconds\n",
                        "69 epoch: avg losses 41538.9216 27.6738 0.0000 0.0000 elapsed 6.0617 seconds\n",
                        "70 epoch: avg losses 41426.3867 25.9810 0.0000 0.0000 elapsed 6.0238 seconds\n",
                        "71 epoch: avg losses 41323.5535 24.8627 0.0000 0.0000 elapsed 6.0322 seconds\n",
                        "72 epoch: avg losses 41216.3551 23.3411 0.0000 0.0000 elapsed 6.0124 seconds\n",
                        "73 epoch: avg losses 41092.9116 21.7989 0.0000 0.0000 elapsed 5.9922 seconds\n",
                        "74 epoch: avg losses 40994.1642 19.0429 0.0000 0.0000 elapsed 6.0444 seconds\n",
                        "75 epoch: avg losses 40870.5208 19.1658 0.0000 0.0000 elapsed 6.0134 seconds\n",
                        "76 epoch: avg losses 40770.0554 17.5810 0.0000 0.0000 elapsed 6.0747 seconds\n",
                        "77 epoch: avg losses 40656.0248 17.0854 0.0000 0.0000 elapsed 6.0323 seconds\n",
                        "78 epoch: avg losses 40541.8494 15.6472 0.0000 0.0000 elapsed 6.0056 seconds\n",
                        "79 epoch: avg losses 40427.8912 15.0543 0.0000 0.0000 elapsed 6.0533 seconds\n",
                        "80 epoch: avg losses 40335.6447 15.2851 0.0000 0.0000 elapsed 5.9842 seconds\n",
                        "81 epoch: avg losses 40226.3912 12.5663 0.0000 0.0000 elapsed 6.0425 seconds\n",
                        "82 epoch: avg losses 40100.3739 12.3168 0.0000 0.0000 elapsed 6.0105 seconds\n",
                        "83 epoch: avg losses 39989.6322 12.1417 0.0000 0.0000 elapsed 6.0284 seconds\n",
                        "84 epoch: avg losses 39909.1846 12.0956 0.0000 0.0000 elapsed 5.8436 seconds\n",
                        "85 epoch: avg losses 39809.1381 10.9794 0.0000 0.0000 elapsed 6.0327 seconds\n",
                        "86 epoch: avg losses 39725.3811 10.4665 0.0000 0.0000 elapsed 6.0353 seconds\n",
                        "87 epoch: avg losses 39624.1226 9.4619 0.0000 0.0000 elapsed 6.0327 seconds\n",
                        "88 epoch: avg losses 39527.5253 10.4278 0.0000 0.0000 elapsed 6.0452 seconds\n",
                        "89 epoch: avg losses 39433.7466 9.0433 0.0000 0.0000 elapsed 6.0633 seconds\n",
                        "90 epoch: avg losses 39352.2005 8.7626 0.0000 0.0000 elapsed 6.0252 seconds\n",
                        "91 epoch: avg losses 39253.9472 8.4159 0.0000 0.0000 elapsed 6.0341 seconds\n",
                        "92 epoch: avg losses 39164.4125 8.1454 0.0000 0.0000 elapsed 6.0332 seconds\n",
                        "93 epoch: avg losses 39065.7537 13.0205 0.0000 0.0000 elapsed 6.0906 seconds\n",
                        "94 epoch: avg losses 38964.9303 7.3471 0.0000 0.0000 elapsed 6.0491 seconds\n",
                        "95 epoch: avg losses 38896.3838 6.9915 0.0000 0.0000 elapsed 6.0437 seconds\n",
                        "96 epoch: avg losses 38797.6699 6.9333 0.0000 0.0000 elapsed 6.0355 seconds\n",
                        "97 epoch: avg losses 38703.5223 6.6703 0.0000 0.0000 elapsed 6.0350 seconds\n",
                        "98 epoch: avg losses 38620.9480 6.7207 0.0000 0.0000 elapsed 6.0055 seconds\n",
                        "99 epoch: avg losses 38525.3577 6.9181 0.0000 0.0000 elapsed 6.0304 seconds\n",
                        "100 epoch: avg losses 38436.2057 6.4154 0.0000 0.0000 elapsed 6.0367 seconds\n",
                        "101 epoch: avg losses 38350.7851 6.4352 0.0000 0.0000 elapsed 6.0345 seconds\n",
                        "102 epoch: avg losses 38253.5198 5.8057 0.0000 0.0000 elapsed 6.0442 seconds\n",
                        "103 epoch: avg losses 38166.8311 5.7200 0.0000 0.0000 elapsed 6.0534 seconds\n",
                        "104 epoch: avg losses 38104.4267 5.5108 0.0000 0.0000 elapsed 6.0949 seconds\n",
                        "105 epoch: avg losses 38019.5151 5.3941 0.0000 0.0000 elapsed 6.0259 seconds\n",
                        "106 epoch: avg losses 37939.2658 5.6432 0.0000 0.0000 elapsed 6.0336 seconds\n",
                        "107 epoch: avg losses 37853.3435 5.0559 0.0000 0.0000 elapsed 6.0435 seconds\n",
                        "108 epoch: avg losses 37791.5707 5.1811 0.0000 0.0000 elapsed 6.0525 seconds\n",
                        "109 epoch: avg losses 37695.4044 5.0829 0.0000 0.0000 elapsed 6.0864 seconds\n",
                        "110 epoch: avg losses 37622.8512 4.6918 0.0000 0.0000 elapsed 5.9322 seconds\n",
                        "111 epoch: avg losses 37548.4775 5.9542 0.0000 0.0000 elapsed 6.0552 seconds\n",
                        "112 epoch: avg losses 37467.7950 13.7422 0.0000 0.0000 elapsed 6.0928 seconds\n",
                        "113 epoch: avg losses 37393.0649 4.3489 0.0000 0.0000 elapsed 6.0354 seconds\n",
                        "114 epoch: avg losses 37318.0072 4.2098 0.0000 0.0000 elapsed 6.0127 seconds\n",
                        "115 epoch: avg losses 37247.4578 4.1564 0.0000 0.0000 elapsed 5.9793 seconds\n",
                        "116 epoch: avg losses 37163.8828 3.9785 0.0000 0.0000 elapsed 6.0790 seconds\n",
                        "117 epoch: avg losses 37084.6041 4.0888 0.0000 0.0000 elapsed 6.0341 seconds\n",
                        "118 epoch: avg losses 37013.4704 3.9727 0.0000 0.0000 elapsed 6.0339 seconds\n",
                        "119 epoch: avg losses 36950.1280 3.9003 0.0000 0.0000 elapsed 6.0440 seconds\n",
                        "120 epoch: avg losses 36869.6262 3.9522 0.0000 0.0000 elapsed 6.0458 seconds\n",
                        "121 epoch: avg losses 36792.8702 3.6225 0.0000 0.0000 elapsed 6.0449 seconds\n",
                        "122 epoch: avg losses 36711.4945 3.7817 0.0000 0.0000 elapsed 6.0628 seconds\n",
                        "123 epoch: avg losses 36656.6209 3.5874 0.0000 0.0000 elapsed 6.0661 seconds\n",
                        "124 epoch: avg losses 36583.4696 3.6529 0.0000 0.0000 elapsed 6.0640 seconds\n",
                        "125 epoch: avg losses 36508.9141 3.7898 0.0000 0.0000 elapsed 6.0460 seconds\n",
                        "126 epoch: avg losses 36444.5611 3.4783 0.0000 0.0000 elapsed 6.0018 seconds\n",
                        "127 epoch: avg losses 36383.4594 3.3666 0.0000 0.0000 elapsed 6.0360 seconds\n",
                        "128 epoch: avg losses 36320.6467 3.1846 0.0000 0.0000 elapsed 6.0432 seconds\n",
                        "129 epoch: avg losses 36257.6331 3.2239 0.0000 0.0000 elapsed 6.0460 seconds\n",
                        "130 epoch: avg losses 36182.8564 3.3521 0.0000 0.0000 elapsed 6.0348 seconds\n",
                        "131 epoch: avg losses 36121.6364 3.0481 0.0000 0.0000 elapsed 6.0246 seconds\n",
                        "132 epoch: avg losses 36057.5449 3.1065 0.0000 0.0000 elapsed 6.0243 seconds\n",
                        "133 epoch: avg losses 35981.8076 2.8502 0.0000 0.0000 elapsed 6.0134 seconds\n",
                        "134 epoch: avg losses 35935.2599 2.6477 0.0000 0.0000 elapsed 6.0020 seconds\n",
                        "135 epoch: avg losses 35883.6985 3.1272 0.0000 0.0000 elapsed 6.0532 seconds\n",
                        "136 epoch: avg losses 35798.3281 3.0528 0.0000 0.0000 elapsed 6.0541 seconds\n",
                        "137 epoch: avg losses 35745.5955 2.8460 0.0000 0.0000 elapsed 6.0807 seconds\n",
                        "138 epoch: avg losses 35681.2568 2.6719 0.0000 0.0000 elapsed 6.0507 seconds\n",
                        "139 epoch: avg losses 35603.4048 2.5990 0.0000 0.0000 elapsed 6.0342 seconds\n",
                        "140 epoch: avg losses 35555.4901 2.4729 0.0000 0.0000 elapsed 6.0494 seconds\n",
                        "141 epoch: avg losses 35492.0551 2.7970 0.0000 0.0000 elapsed 6.0204 seconds\n",
                        "142 epoch: avg losses 35411.6887 2.3426 0.0000 0.0000 elapsed 6.0325 seconds\n",
                        "143 epoch: avg losses 35360.5734 2.3318 0.0000 0.0000 elapsed 6.0449 seconds\n",
                        "144 epoch: avg losses 35301.8825 2.5742 0.0000 0.0000 elapsed 6.0333 seconds\n",
                        "145 epoch: avg losses 35251.5432 2.4007 0.0000 0.0000 elapsed 6.0541 seconds\n",
                        "146 epoch: avg losses 35200.1579 2.4138 0.0000 0.0000 elapsed 6.0432 seconds\n",
                        "147 epoch: avg losses 35150.1130 2.3694 0.0000 0.0000 elapsed 6.0127 seconds\n",
                        "148 epoch: avg losses 35074.9566 2.4152 0.0000 0.0000 elapsed 6.0241 seconds\n",
                        "149 epoch: avg losses 35032.5548 2.0193 0.0000 0.0000 elapsed 6.0431 seconds\n",
                        "150 epoch: avg losses 34984.1180 2.0612 0.0000 0.0000 elapsed 6.0599 seconds\n",
                        "151 epoch: avg losses 34930.7403 2.1538 0.0000 0.0000 elapsed 6.0463 seconds\n",
                        "152 epoch: avg losses 34866.0619 2.0975 0.0000 0.0000 elapsed 6.0654 seconds\n",
                        "153 epoch: avg losses 34812.3533 1.9910 0.0000 0.0000 elapsed 6.0555 seconds\n",
                        "154 epoch: avg losses 34767.3351 2.0892 0.0000 0.0000 elapsed 6.0645 seconds\n",
                        "155 epoch: avg losses 34705.0176 2.0999 0.0000 0.0000 elapsed 6.0628 seconds\n",
                        "156 epoch: avg losses 34643.8096 1.9198 0.0000 0.0000 elapsed 6.0345 seconds\n",
                        "157 epoch: avg losses 34592.0630 1.8774 0.0000 0.0000 elapsed 6.0439 seconds\n",
                        "158 epoch: avg losses 34548.1641 1.9836 0.0000 0.0000 elapsed 6.0524 seconds\n",
                        "159 epoch: avg losses 34498.5015 1.9658 0.0000 0.0000 elapsed 6.0551 seconds\n",
                        "160 epoch: avg losses 34446.5405 1.8932 0.0000 0.0000 elapsed 6.0482 seconds\n",
                        "161 epoch: avg losses 34384.1272 1.8341 0.0000 0.0000 elapsed 6.0514 seconds\n",
                        "162 epoch: avg losses 34329.9195 1.8820 0.0000 0.0000 elapsed 6.1296 seconds\n",
                        "163 epoch: avg losses 34272.8123 1.6980 0.0000 0.0000 elapsed 6.0246 seconds\n",
                        "164 epoch: avg losses 34227.5881 1.7097 0.0000 0.0000 elapsed 6.0428 seconds\n",
                        "165 epoch: avg losses 34182.6117 1.6697 0.0000 0.0000 elapsed 6.0532 seconds\n",
                        "166 epoch: avg losses 34141.2813 1.7526 0.0000 0.0000 elapsed 6.0474 seconds\n",
                        "167 epoch: avg losses 34095.9382 1.5729 0.0000 0.0000 elapsed 6.0486 seconds\n",
                        "168 epoch: avg losses 34052.3700 1.6782 0.0000 0.0000 elapsed 6.0127 seconds\n",
                        "169 epoch: avg losses 34004.4520 1.5225 0.0000 0.0000 elapsed 6.0555 seconds\n",
                        "170 epoch: avg losses 33953.6188 1.7198 0.0000 0.0000 elapsed 6.0112 seconds\n",
                        "171 epoch: avg losses 33924.4347 1.5505 0.0000 0.0000 elapsed 6.0606 seconds\n",
                        "172 epoch: avg losses 33862.9426 1.4486 0.0000 0.0000 elapsed 6.1398 seconds\n",
                        "173 epoch: avg losses 33816.0143 1.5140 0.0000 0.0000 elapsed 6.0351 seconds\n",
                        "174 epoch: avg losses 33765.0777 1.4616 0.0000 0.0000 elapsed 6.0434 seconds\n",
                        "175 epoch: avg losses 33728.9421 1.4959 0.0000 0.0000 elapsed 6.1113 seconds\n",
                        "176 epoch: avg losses 33683.1944 1.6435 0.0000 0.0000 elapsed 6.0722 seconds\n",
                        "177 epoch: avg losses 33639.7404 1.3286 0.0000 0.0000 elapsed 6.0342 seconds\n",
                        "178 epoch: avg losses 33592.5757 1.3736 0.0000 0.0000 elapsed 6.0322 seconds\n",
                        "179 epoch: avg losses 33550.4430 1.4572 0.0000 0.0000 elapsed 6.0209 seconds\n",
                        "180 epoch: avg losses 33496.9863 1.3567 0.0000 0.0000 elapsed 6.0328 seconds\n",
                        "181 epoch: avg losses 33459.2931 1.3818 0.0000 0.0000 elapsed 6.0333 seconds\n",
                        "182 epoch: avg losses 33412.7480 1.3059 0.0000 0.0000 elapsed 6.0514 seconds\n",
                        "183 epoch: avg losses 33370.1254 1.2331 0.0000 0.0000 elapsed 6.0331 seconds\n",
                        "184 epoch: avg losses 33328.8206 1.3557 0.0000 0.0000 elapsed 6.0640 seconds\n",
                        "185 epoch: avg losses 33292.7677 1.2939 0.0000 0.0000 elapsed 6.0133 seconds\n",
                        "186 epoch: avg losses 33247.1021 1.2844 0.0000 0.0000 elapsed 6.0736 seconds\n",
                        "187 epoch: avg losses 33221.5284 1.3082 0.0000 0.0000 elapsed 6.0228 seconds\n",
                        "188 epoch: avg losses 33175.7196 1.2843 0.0000 0.0000 elapsed 6.0733 seconds\n",
                        "189 epoch: avg losses 33133.3268 1.2695 0.0000 0.0000 elapsed 6.0232 seconds\n",
                        "190 epoch: avg losses 33090.4362 1.2940 0.0000 0.0000 elapsed 6.0532 seconds\n",
                        "191 epoch: avg losses 33058.1197 1.3021 0.0000 0.0000 elapsed 6.0628 seconds\n",
                        "192 epoch: avg losses 33010.9831 1.2711 0.0000 0.0000 elapsed 6.0645 seconds\n",
                        "193 epoch: avg losses 32975.3395 1.2275 0.0000 0.0000 elapsed 6.0032 seconds\n",
                        "194 epoch: avg losses 32933.7306 1.2550 0.0000 0.0000 elapsed 6.0528 seconds\n",
                        "195 epoch: avg losses 32891.5354 1.1267 0.0000 0.0000 elapsed 6.0341 seconds\n",
                        "196 epoch: avg losses 32860.3803 1.4169 0.0000 0.0000 elapsed 6.0528 seconds\n",
                        "197 epoch: avg losses 32818.6721 1.2722 0.0000 0.0000 elapsed 6.0537 seconds\n",
                        "198 epoch: avg losses 32793.8755 1.2349 0.0000 0.0000 elapsed 6.0147 seconds\n",
                        "199 epoch: avg losses 32750.8217 1.1499 0.0000 0.0000 elapsed 6.0426 seconds\n",
                        "200 epoch: avg losses 32706.7521 1.0826 0.0000 0.0000 elapsed 6.0639 seconds\n",
                        "201 epoch: avg losses 32669.0838 1.0486 0.0000 0.0000 elapsed 6.0427 seconds\n",
                        "202 epoch: avg losses 32636.0986 1.0876 0.0000 0.0000 elapsed 6.0121 seconds\n",
                        "203 epoch: avg losses 32595.2654 1.0395 0.0000 0.0000 elapsed 6.0416 seconds\n",
                        "204 epoch: avg losses 32565.1097 1.0585 0.0000 0.0000 elapsed 6.0429 seconds\n",
                        "205 epoch: avg losses 32533.3174 1.0803 0.0000 0.0000 elapsed 6.0248 seconds\n",
                        "206 epoch: avg losses 32503.4091 1.0794 0.0000 0.0000 elapsed 6.0499 seconds\n",
                        "207 epoch: avg losses 32477.0122 1.0722 0.0000 0.0000 elapsed 6.0955 seconds\n",
                        "208 epoch: avg losses 32421.8335 1.1513 0.0000 0.0000 elapsed 6.0306 seconds\n",
                        "209 epoch: avg losses 32388.7850 1.0201 0.0000 0.0000 elapsed 6.0939 seconds\n",
                        "210 epoch: avg losses 32356.3632 1.0398 0.0000 0.0000 elapsed 6.0426 seconds\n",
                        "211 epoch: avg losses 32329.9648 1.0295 0.0000 0.0000 elapsed 6.0434 seconds\n",
                        "212 epoch: avg losses 32291.2711 1.0011 0.0000 0.0000 elapsed 6.0222 seconds\n",
                        "213 epoch: avg losses 32266.6431 1.0630 0.0000 0.0000 elapsed 6.0631 seconds\n",
                        "214 epoch: avg losses 32235.9257 0.9656 0.0000 0.0000 elapsed 6.0530 seconds\n",
                        "215 epoch: avg losses 32188.4510 0.9647 0.0000 0.0000 elapsed 6.0431 seconds\n",
                        "216 epoch: avg losses 32166.7571 0.9727 0.0000 0.0000 elapsed 6.0227 seconds\n",
                        "217 epoch: avg losses 32131.8187 1.0158 0.0000 0.0000 elapsed 6.0523 seconds\n",
                        "218 epoch: avg losses 32105.5461 0.9221 0.0000 0.0000 elapsed 6.0440 seconds\n",
                        "219 epoch: avg losses 32068.2082 0.9675 0.0000 0.0000 elapsed 6.0499 seconds\n",
                        "220 epoch: avg losses 32037.4135 1.0019 0.0000 0.0000 elapsed 6.0261 seconds\n",
                        "221 epoch: avg losses 32002.0093 0.9214 0.0000 0.0000 elapsed 6.0236 seconds\n",
                        "222 epoch: avg losses 31961.0182 0.8659 0.0000 0.0000 elapsed 6.0542 seconds\n",
                        "223 epoch: avg losses 31931.0687 0.8625 0.0000 0.0000 elapsed 6.0535 seconds\n",
                        "224 epoch: avg losses 31910.4136 0.8916 0.0000 0.0000 elapsed 6.0617 seconds\n",
                        "225 epoch: avg losses 31879.3845 0.8695 0.0000 0.0000 elapsed 6.0543 seconds\n",
                        "226 epoch: avg losses 31852.5844 0.8660 0.0000 0.0000 elapsed 6.0936 seconds\n",
                        "227 epoch: avg losses 31836.4410 0.8729 0.0000 0.0000 elapsed 6.0358 seconds\n",
                        "228 epoch: avg losses 31799.9102 0.8494 0.0000 0.0000 elapsed 6.0099 seconds\n",
                        "229 epoch: avg losses 31769.3365 0.8572 0.0000 0.0000 elapsed 6.0467 seconds\n",
                        "230 epoch: avg losses 31740.9116 0.9224 0.0000 0.0000 elapsed 6.0407 seconds\n",
                        "231 epoch: avg losses 31710.5273 0.8100 0.0000 0.0000 elapsed 6.0727 seconds\n",
                        "232 epoch: avg losses 31685.6501 0.8301 0.0000 0.0000 elapsed 6.0331 seconds\n",
                        "233 epoch: avg losses 31663.6822 0.8602 0.0000 0.0000 elapsed 6.0858 seconds\n",
                        "234 epoch: avg losses 31623.0865 0.8182 0.0000 0.0000 elapsed 6.0239 seconds\n",
                        "235 epoch: avg losses 31597.6089 0.9221 0.0000 0.0000 elapsed 6.0618 seconds\n",
                        "236 epoch: avg losses 31584.5177 0.8094 0.0000 0.0000 elapsed 6.1042 seconds\n",
                        "237 epoch: avg losses 31555.1114 0.8087 0.0000 0.0000 elapsed 6.0344 seconds\n",
                        "238 epoch: avg losses 31527.0950 0.7838 0.0000 0.0000 elapsed 6.0534 seconds\n",
                        "239 epoch: avg losses 31489.9180 0.8728 0.0000 0.0000 elapsed 6.0437 seconds\n",
                        "240 epoch: avg losses 31467.6235 0.8063 0.0000 0.0000 elapsed 6.0845 seconds\n",
                        "241 epoch: avg losses 31443.3291 0.7620 0.0000 0.0000 elapsed 6.0444 seconds\n",
                        "242 epoch: avg losses 31410.6879 0.7382 0.0000 0.0000 elapsed 6.0331 seconds\n",
                        "243 epoch: avg losses 31376.5376 0.7757 0.0000 0.0000 elapsed 6.0128 seconds\n",
                        "244 epoch: avg losses 31359.8562 0.7438 0.0000 0.0000 elapsed 6.0437 seconds\n",
                        "245 epoch: avg losses 31342.8175 0.7819 0.0000 0.0000 elapsed 6.0939 seconds\n",
                        "246 epoch: avg losses 31305.1199 0.8318 0.0000 0.0000 elapsed 6.0768 seconds\n",
                        "247 epoch: avg losses 31297.0807 0.7750 0.0000 0.0000 elapsed 6.0419 seconds\n",
                        "248 epoch: avg losses 31260.6232 0.7479 0.0000 0.0000 elapsed 6.0521 seconds\n",
                        "249 epoch: avg losses 31246.1420 0.7507 0.0000 0.0000 elapsed 6.0332 seconds\n",
                        "250 epoch: avg losses 31207.6609 0.7342 0.0000 0.0000 elapsed 6.0444 seconds\n",
                        "251 epoch: avg losses 31202.0890 0.6921 0.0000 0.0000 elapsed 6.0462 seconds\n",
                        "252 epoch: avg losses 31175.5298 0.7016 0.0000 0.0000 elapsed 6.0307 seconds\n",
                        "253 epoch: avg losses 31139.3355 0.7308 0.0000 0.0000 elapsed 6.0729 seconds\n",
                        "254 epoch: avg losses 31113.1925 0.7293 0.0000 0.0000 elapsed 6.0262 seconds\n",
                        "255 epoch: avg losses 31107.3016 0.7166 0.0000 0.0000 elapsed 6.0309 seconds\n",
                        "256 epoch: avg losses 31088.0648 0.8067 0.0000 0.0000 elapsed 6.0441 seconds\n",
                        "257 epoch: avg losses 31054.5322 0.6758 0.0000 0.0000 elapsed 6.0231 seconds\n",
                        "258 epoch: avg losses 31026.2967 0.6785 0.0000 0.0000 elapsed 6.0447 seconds\n",
                        "259 epoch: avg losses 31011.1148 0.6900 0.0000 0.0000 elapsed 6.0639 seconds\n",
                        "260 epoch: avg losses 30987.7756 0.6845 0.0000 0.0000 elapsed 6.0734 seconds\n",
                        "261 epoch: avg losses 30965.1648 0.6574 0.0000 0.0000 elapsed 6.0237 seconds\n",
                        "262 epoch: avg losses 30941.7832 0.7505 0.0000 0.0000 elapsed 6.0406 seconds\n",
                        "263 epoch: avg losses 30914.4555 0.6539 0.0000 0.0000 elapsed 6.0327 seconds\n",
                        "264 epoch: avg losses 30899.2943 0.6522 0.0000 0.0000 elapsed 6.0327 seconds\n",
                        "265 epoch: avg losses 30871.3623 0.6710 0.0000 0.0000 elapsed 6.0445 seconds\n",
                        "266 epoch: avg losses 30866.4992 0.6977 0.0000 0.0000 elapsed 6.0431 seconds\n",
                        "267 epoch: avg losses 30838.5736 0.6722 0.0000 0.0000 elapsed 6.0382 seconds\n",
                        "268 epoch: avg losses 30814.9937 0.6686 0.0000 0.0000 elapsed 6.0409 seconds\n",
                        "269 epoch: avg losses 30800.3733 0.6461 0.0000 0.0000 elapsed 6.0162 seconds\n",
                        "270 epoch: avg losses 30780.4105 0.6340 0.0000 0.0000 elapsed 6.0236 seconds\n",
                        "271 epoch: avg losses 30760.3334 0.6192 0.0000 0.0000 elapsed 6.0788 seconds\n",
                        "272 epoch: avg losses 30723.6282 0.6498 0.0000 0.0000 elapsed 6.0376 seconds\n",
                        "273 epoch: avg losses 30725.9838 0.6701 0.0000 0.0000 elapsed 6.0358 seconds\n",
                        "274 epoch: avg losses 30702.5339 0.6410 0.0000 0.0000 elapsed 6.0521 seconds\n",
                        "275 epoch: avg losses 30678.0299 0.6291 0.0000 0.0000 elapsed 6.0351 seconds\n",
                        "276 epoch: avg losses 30652.1465 0.5847 0.0000 0.0000 elapsed 6.0454 seconds\n",
                        "277 epoch: avg losses 30633.9123 0.6026 0.0000 0.0000 elapsed 6.0428 seconds\n",
                        "278 epoch: avg losses 30609.7086 0.6476 0.0000 0.0000 elapsed 6.0236 seconds\n",
                        "279 epoch: avg losses 30593.3079 0.5896 0.0000 0.0000 elapsed 6.0743 seconds\n",
                        "280 epoch: avg losses 30578.0887 0.5926 0.0000 0.0000 elapsed 6.0342 seconds\n",
                        "281 epoch: avg losses 30546.2726 0.5919 0.0000 0.0000 elapsed 6.0145 seconds\n",
                        "282 epoch: avg losses 30532.6118 0.5610 0.0000 0.0000 elapsed 6.0131 seconds\n",
                        "283 epoch: avg losses 30514.3150 0.5575 0.0000 0.0000 elapsed 6.0023 seconds\n",
                        "284 epoch: avg losses 30502.8073 0.5829 0.0000 0.0000 elapsed 6.0742 seconds\n",
                        "285 epoch: avg losses 30492.5122 0.5929 0.0000 0.0000 elapsed 6.0044 seconds\n",
                        "286 epoch: avg losses 30471.1238 0.5888 0.0000 0.0000 elapsed 6.0942 seconds\n",
                        "287 epoch: avg losses 30449.4580 0.5890 0.0000 0.0000 elapsed 6.0539 seconds\n",
                        "288 epoch: avg losses 30429.6616 0.5787 0.0000 0.0000 elapsed 6.0139 seconds\n",
                        "289 epoch: avg losses 30413.5317 0.5872 0.0000 0.0000 elapsed 6.0151 seconds\n",
                        "290 epoch: avg losses 30399.6524 0.5859 0.0000 0.0000 elapsed 6.0334 seconds\n",
                        "291 epoch: avg losses 30388.8923 0.5874 0.0000 0.0000 elapsed 6.0572 seconds\n",
                        "292 epoch: avg losses 30360.0926 0.5762 0.0000 0.0000 elapsed 6.0309 seconds\n",
                        "293 epoch: avg losses 30342.8180 0.6106 0.0000 0.0000 elapsed 6.0238 seconds\n",
                        "294 epoch: avg losses 30339.5730 0.6024 0.0000 0.0000 elapsed 6.0428 seconds\n",
                        "295 epoch: avg losses 30303.5599 0.5959 0.0000 0.0000 elapsed 6.0351 seconds\n",
                        "296 epoch: avg losses 30302.7271 0.5516 0.0000 0.0000 elapsed 6.0655 seconds\n",
                        "297 epoch: avg losses 30279.9249 0.5460 0.0000 0.0000 elapsed 6.0228 seconds\n",
                        "298 epoch: avg losses 30274.6437 0.5989 0.0000 0.0000 elapsed 6.0424 seconds\n",
                        "299 epoch: avg losses 30251.3074 0.5944 0.0000 0.0000 elapsed 6.4460 seconds\n",
                        "300 epoch: avg losses 30236.4769 0.5904 0.0000 0.0000 elapsed 6.5339 seconds\n",
                        "best validation accuracy 0.0000\n",
                        "F1: 0.0000(macro) 0.0000(weighted) \n",
                        "precision 0.0000 recall 0.0000 \n",
                        "mcc 0.0000\n"
                    ]
                }
            ],
            "source": [
                "%%bash\n",
                "#conda activate pyro2\n",
                "\n",
                "CUDA_VISIBLE_DEVICES=0 python scClassifier.py --sup-data-file /home/zfeng/zfeng/scClassifier_Ex/pbmcsca.mtx \\\n",
                "                        --sup-label-file /home/zfeng/zfeng/scClassifier_Ex/pbmcsca_factors.txt \\\n",
                "                        -lr 0.0001 \\\n",
                "                        -n 300 \\\n",
                "                        -bs 100 \\\n",
                "                        --aux-loss \\\n",
                "                        -alm 100 \\\n",
                "                        -64 \\\n",
                "                        --jit \\\n",
                "                        --cuda \\\n",
                "                        -zi \\\n",
                "                        -likeli negbinomial \\\n",
                "                        -dirichlet \\\n",
                "                        --label-type onehot \\\n",
                "                        --validation-fold 0 \\\n",
                "                        --save-model pbmcsca.pth 2>&1 | tee pbmcsca.log\n",
                "\n",
                "#conda deactivate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy.io import mmread\n",
                "from scClassifier import scClassifier\n",
                "from utils.scdata_cached import setup_data_loader, SingleCellCached\n",
                "\n",
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "from sklearn.metrics import accuracy_score, f1_score\n",
                "\n",
                "import datatable as dt\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "ModelPath = 'pbmcsca.pth'\n",
                "DataPath='/home/zfeng/zfeng/scClassifier_Ex/pbmcsca.mtx'\n",
                "LabelPath='/home/zfeng/zfeng/scClassifier_Ex/pbmcsca_celltype.txt'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load model\n",
                "model = torch.load(ModelPath)\n",
                "\n",
                "use_float64 = True\n",
                "use_cuda = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load data\n",
                "batch_size = 100\n",
                "\n",
                "data_cached = SingleCellCached(DataPath, LabelPath, 'condition', use_cuda=use_cuda, use_float64 = use_float64)\n",
                "data_loader = DataLoader(data_cached, batch_size = batch_size, shuffle = False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# predict conditions\n",
                "embeds = []\n",
                "exprs = []\n",
                "# use the appropriate data loader\n",
                "for xs,ys in data_loader:\n",
                "    # use classification function to compute all predictions for each batch\n",
                "\n",
                "    if use_cuda:\n",
                "        xs = xs.cuda()\n",
                "        ys = ys.cuda()\n",
                "\n",
                "    zs = model.latent_embedding(xs)\n",
                "    expr = model.mute_expression(xs, mute_label_names=[\"10x Chromium (v2)\",\"10x Chromium (v2) A\",\"10x Chromium (v2) B\",\"10x Chromium (v3)\",\"CEL-Seq2\",\"Drop-seq\",\"Seq-Well\",\"Smart-seq2\",\"inDrops\"], mute_noise=False)\n",
                "\n",
                "    if use_cuda:\n",
                "        zs = zs.cpu().detach().numpy()\n",
                "        expr = expr.cpu().detach().numpy()\n",
                "    else:\n",
                "        zs = zs.detach().numpy()\n",
                "        expr = expr.detach().numpy()\n",
                "\n",
                "    embeds.append(zs)\n",
                "    exprs.append(expr)\n",
                "\n",
                "\n",
                "embeds = np.concatenate(embeds, axis=0)\n",
                "exprs = np.concatenate(exprs, axis=0)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(14890, 1)"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "cells = pd.read_csv('/home/zfeng/zfeng/scClassifier_Ex/pbmcsca_cell.txt', header=None, index_col=None)\n",
                "genes = pd.read_csv('/home/zfeng/zfeng/scClassifier_Ex/pbmcsca_gene.txt', header=None, index_col=None)\n",
                "cells.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>CLPS</th>\n",
                            "      <th>CPB1</th>\n",
                            "      <th>MT1G</th>\n",
                            "      <th>REG3A</th>\n",
                            "      <th>COL1A2</th>\n",
                            "      <th>MMP1</th>\n",
                            "      <th>CRP</th>\n",
                            "      <th>CELA3B</th>\n",
                            "      <th>CELA3A</th>\n",
                            "      <th>CPA1</th>\n",
                            "      <th>...</th>\n",
                            "      <th>AC008269.1</th>\n",
                            "      <th>AC079061.1</th>\n",
                            "      <th>PQLC2L</th>\n",
                            "      <th>DOCK2</th>\n",
                            "      <th>OPRK1</th>\n",
                            "      <th>ZNF878</th>\n",
                            "      <th>OPRPN</th>\n",
                            "      <th>NEMP2</th>\n",
                            "      <th>CCDC81</th>\n",
                            "      <th>TEPSIN</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>D101_5</th>\n",
                            "      <td>0.000487</td>\n",
                            "      <td>0.006616</td>\n",
                            "      <td>0.000219</td>\n",
                            "      <td>0.002253</td>\n",
                            "      <td>0.000213</td>\n",
                            "      <td>0.000039</td>\n",
                            "      <td>0.000120</td>\n",
                            "      <td>0.000166</td>\n",
                            "      <td>0.000319</td>\n",
                            "      <td>0.000496</td>\n",
                            "      <td>...</td>\n",
                            "      <td>4.478372e-07</td>\n",
                            "      <td>3.688737e-08</td>\n",
                            "      <td>0.000341</td>\n",
                            "      <td>1.208603e-06</td>\n",
                            "      <td>1.001594e-05</td>\n",
                            "      <td>3.687854e-06</td>\n",
                            "      <td>1.156372e-06</td>\n",
                            "      <td>0.000126</td>\n",
                            "      <td>0.000087</td>\n",
                            "      <td>9.953282e-06</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>D101_7</th>\n",
                            "      <td>0.003216</td>\n",
                            "      <td>0.005496</td>\n",
                            "      <td>0.002627</td>\n",
                            "      <td>0.006814</td>\n",
                            "      <td>0.000389</td>\n",
                            "      <td>0.000499</td>\n",
                            "      <td>0.000631</td>\n",
                            "      <td>0.003664</td>\n",
                            "      <td>0.005937</td>\n",
                            "      <td>0.006604</td>\n",
                            "      <td>...</td>\n",
                            "      <td>3.203163e-08</td>\n",
                            "      <td>6.645654e-08</td>\n",
                            "      <td>0.000041</td>\n",
                            "      <td>2.934307e-06</td>\n",
                            "      <td>1.842269e-06</td>\n",
                            "      <td>1.458249e-05</td>\n",
                            "      <td>2.833236e-06</td>\n",
                            "      <td>0.000069</td>\n",
                            "      <td>0.000035</td>\n",
                            "      <td>3.761142e-07</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>D101_10</th>\n",
                            "      <td>0.000433</td>\n",
                            "      <td>0.000976</td>\n",
                            "      <td>0.000129</td>\n",
                            "      <td>0.002376</td>\n",
                            "      <td>0.000322</td>\n",
                            "      <td>0.000159</td>\n",
                            "      <td>0.000040</td>\n",
                            "      <td>0.000264</td>\n",
                            "      <td>0.000665</td>\n",
                            "      <td>0.000974</td>\n",
                            "      <td>...</td>\n",
                            "      <td>6.038206e-08</td>\n",
                            "      <td>1.214390e-08</td>\n",
                            "      <td>0.000003</td>\n",
                            "      <td>6.543814e-07</td>\n",
                            "      <td>1.940952e-06</td>\n",
                            "      <td>4.005412e-06</td>\n",
                            "      <td>2.570803e-06</td>\n",
                            "      <td>0.000087</td>\n",
                            "      <td>0.000172</td>\n",
                            "      <td>1.687440e-06</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>D101_13</th>\n",
                            "      <td>0.000476</td>\n",
                            "      <td>0.003881</td>\n",
                            "      <td>0.000139</td>\n",
                            "      <td>0.001694</td>\n",
                            "      <td>0.000434</td>\n",
                            "      <td>0.000175</td>\n",
                            "      <td>0.000033</td>\n",
                            "      <td>0.000229</td>\n",
                            "      <td>0.000652</td>\n",
                            "      <td>0.000935</td>\n",
                            "      <td>...</td>\n",
                            "      <td>7.253705e-08</td>\n",
                            "      <td>3.011997e-08</td>\n",
                            "      <td>0.000013</td>\n",
                            "      <td>7.043941e-06</td>\n",
                            "      <td>3.390564e-06</td>\n",
                            "      <td>6.146250e-07</td>\n",
                            "      <td>1.120850e-08</td>\n",
                            "      <td>0.000079</td>\n",
                            "      <td>0.000087</td>\n",
                            "      <td>5.395948e-09</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>D101_14</th>\n",
                            "      <td>0.000509</td>\n",
                            "      <td>0.000922</td>\n",
                            "      <td>0.000158</td>\n",
                            "      <td>0.001945</td>\n",
                            "      <td>0.000176</td>\n",
                            "      <td>0.000052</td>\n",
                            "      <td>0.000009</td>\n",
                            "      <td>0.000141</td>\n",
                            "      <td>0.000476</td>\n",
                            "      <td>0.000857</td>\n",
                            "      <td>...</td>\n",
                            "      <td>1.109188e-06</td>\n",
                            "      <td>1.122901e-06</td>\n",
                            "      <td>0.000012</td>\n",
                            "      <td>2.714067e-06</td>\n",
                            "      <td>1.056993e-07</td>\n",
                            "      <td>1.348281e-06</td>\n",
                            "      <td>6.280447e-09</td>\n",
                            "      <td>0.000102</td>\n",
                            "      <td>0.000137</td>\n",
                            "      <td>1.054907e-07</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>5 rows × 2000 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "             CLPS      CPB1      MT1G     REG3A    COL1A2      MMP1       CRP  \\\n",
                            "D101_5   0.000487  0.006616  0.000219  0.002253  0.000213  0.000039  0.000120   \n",
                            "D101_7   0.003216  0.005496  0.002627  0.006814  0.000389  0.000499  0.000631   \n",
                            "D101_10  0.000433  0.000976  0.000129  0.002376  0.000322  0.000159  0.000040   \n",
                            "D101_13  0.000476  0.003881  0.000139  0.001694  0.000434  0.000175  0.000033   \n",
                            "D101_14  0.000509  0.000922  0.000158  0.001945  0.000176  0.000052  0.000009   \n",
                            "\n",
                            "           CELA3B    CELA3A      CPA1  ...    AC008269.1    AC079061.1  \\\n",
                            "D101_5   0.000166  0.000319  0.000496  ...  4.478372e-07  3.688737e-08   \n",
                            "D101_7   0.003664  0.005937  0.006604  ...  3.203163e-08  6.645654e-08   \n",
                            "D101_10  0.000264  0.000665  0.000974  ...  6.038206e-08  1.214390e-08   \n",
                            "D101_13  0.000229  0.000652  0.000935  ...  7.253705e-08  3.011997e-08   \n",
                            "D101_14  0.000141  0.000476  0.000857  ...  1.109188e-06  1.122901e-06   \n",
                            "\n",
                            "           PQLC2L         DOCK2         OPRK1        ZNF878         OPRPN  \\\n",
                            "D101_5   0.000341  1.208603e-06  1.001594e-05  3.687854e-06  1.156372e-06   \n",
                            "D101_7   0.000041  2.934307e-06  1.842269e-06  1.458249e-05  2.833236e-06   \n",
                            "D101_10  0.000003  6.543814e-07  1.940952e-06  4.005412e-06  2.570803e-06   \n",
                            "D101_13  0.000013  7.043941e-06  3.390564e-06  6.146250e-07  1.120850e-08   \n",
                            "D101_14  0.000012  2.714067e-06  1.056993e-07  1.348281e-06  6.280447e-09   \n",
                            "\n",
                            "            NEMP2    CCDC81        TEPSIN  \n",
                            "D101_5   0.000126  0.000087  9.953282e-06  \n",
                            "D101_7   0.000069  0.000035  3.761142e-07  \n",
                            "D101_10  0.000087  0.000172  1.687440e-06  \n",
                            "D101_13  0.000079  0.000087  5.395948e-09  \n",
                            "D101_14  0.000102  0.000137  1.054907e-07  \n",
                            "\n",
                            "[5 rows x 2000 columns]"
                        ]
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df = pd.DataFrame(exprs, columns=genes[0].values, index=cells[0].values)\n",
                "dt.Frame(df.reset_index()).to_csv('/home/zfeng/zfeng/scClassifier_Ex/pbmcsca_mutate_express.txt')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(14890, 2000)"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df.shape"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9.15 ('pyro')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.15"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "d03eef04c985d8ab4426f825a8dbbd9cffdaab4d8d03e785ebbc93ce5d0d4ff1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
